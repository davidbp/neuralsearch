{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Prepare-vvgish-input-manually\" data-toc-modified-id=\"Prepare-vvgish-input-manually-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Prepare vvgish input manually</a></span></li><li><span><a href=\"#We-can-directly-go-from-path-of-mp3-file-to-input-vggish\" data-toc-modified-id=\"We-can-directly-go-from-path-of-mp3-file-to-input-vggish-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>We can directly go from path of mp3 file to input vggish</a></span></li></ul></li><li><span><a href=\"#Segmenting-the-data\" data-toc-modified-id=\"Segmenting-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Segmenting the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Finding-matches-in-a-dataset\" data-toc-modified-id=\"Finding-matches-in-a-dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Finding matches in a dataset</a></span></li><li><span><a href=\"#Passing-data-thorugh-the-vvgish\" data-toc-modified-id=\"Passing-data-thorugh-the-vvgish-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Passing data thorugh the vvgish</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:41:05.944103Z",
     "start_time": "2021-05-28T13:41:05.925680Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:21.052967Z",
     "start_time": "2021-05-28T13:43:21.008010Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vggish import mel_features\n",
    "from vggish import vggish_input\n",
    "from vggish import vggish_slim\n",
    "import executors\n",
    "\n",
    "import numpy as np\n",
    "import inspect\n",
    "\n",
    "import jina\n",
    "from jina import Document, DocumentArray\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vvgish input manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:21.848116Z",
     "start_time": "2021-05-28T13:43:21.806645Z"
    }
   },
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:22.669630Z",
     "start_time": "2021-05-28T13:43:21.997495Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidbuchaca1/opt/anaconda3/lib/python3.8/site-packages/librosa/core/audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    }
   ],
   "source": [
    "x_audio, sample_rate = librosa.load('data/sample.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:23.205431Z",
     "start_time": "2021-05-28T13:43:22.775203Z"
    }
   },
   "outputs": [],
   "source": [
    "log_mel_examples = vggish_input.waveform_to_examples(x_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:23.363204Z",
     "start_time": "2021-05-28T13:43:23.318112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 96, 64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_mel_examples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can directly go from path of mp3 file to input vggish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:24.511577Z",
     "start_time": "2021-05-28T13:43:23.468863Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidbuchaca1/opt/anaconda3/lib/python3.8/site-packages/librosa/core/audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    }
   ],
   "source": [
    "log_mel_examples = vggish_input.mp3_to_examples('data/sample.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:24.673653Z",
     "start_time": "2021-05-28T13:43:24.625104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 96, 64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_mel_examples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:25.208584Z",
     "start_time": "2021-05-28T13:43:24.774586Z"
    }
   },
   "outputs": [],
   "source": [
    "log_mel_examples = vggish_input.wavfile_to_examples('data/Beethoven_1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:25.361290Z",
     "start_time": "2021-05-28T13:43:25.314044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19, 96, 64), (116736,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_mel_examples.shape, log_mel_examples.flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenting the data\n",
    "\n",
    "The segmenter in audio-example from jina examples does the following at segment time:\n",
    "\n",
    "- uses `read_wav(path_to_wav)` reads a wav file and returns a numpy array (`data`) and a integer (`sample_rate`)\n",
    "\n",
    "- \n",
    "\n",
    "To read the data the code uses `read_wav` to read the data from a path.\n",
    "This function calls `soundfile.read` which returns a numpy array `data`.\n",
    "\n",
    "\n",
    "\n",
    "Example for `data/Beethoven_1.wav`:\n",
    "\n",
    "- data after `soundfile.read` has shape (806912,)\n",
    "\n",
    "- `mel_data=wav2vel(data)` has shape (806912,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:25.895027Z",
     "start_time": "2021-05-28T13:43:25.833830Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806912, 2)\n",
      "(806912,)\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "wav_data, sample_rate = sf.read('data/Beethoven_1.wav', dtype='int16')\n",
    "print(wav_data.shape)\n",
    "wav_data = np.mean(wav_data, axis=1)\n",
    "data = wav_data / sample_rate\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:27.143248Z",
     "start_time": "2021-05-28T13:43:26.328533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 96, 64)\n",
      "(19, 96, 64)\n"
     ]
    }
   ],
   "source": [
    "from vggish.vggish_input import waveform_to_examples\n",
    "\n",
    "aux = waveform_to_examples(data, sample_rate)\n",
    "print(aux.shape)\n",
    "\n",
    "aux = waveform_to_examples(data, sample_rate).squeeze()\n",
    "print(aux.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:27.300024Z",
     "start_time": "2021-05-28T13:43:27.253964Z"
    }
   },
   "outputs": [],
   "source": [
    "segmenter = executors.VggishSegmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:27.442229Z",
     "start_time": "2021-05-28T13:43:27.398354Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def segment(self, docs, *args, **kwargs):\n",
      "\n",
      "        for doc in docs:\n",
      "            data, sample_rate = self.read_wav(doc.uri)\n",
      "            mel_data = self.wav2mel(data, sample_rate)\n",
      "            for idx, blob in enumerate(mel_data):\n",
      "                #self.logger.debug(f'blob: {blob.shape}')\n",
      "                doc.chunks.append(Document(offset=idx, weight=1.0, blob=blob))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(segmenter.segment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each chunk is a Document containing each element from mel_data.\n",
    "\n",
    "In particular this will create as elements as mel_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T13:43:28.285023Z",
     "start_time": "2021-05-28T13:43:28.241500Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding matches in a dataset \n",
    "\n",
    "We have seen that given an audio input we create a numpy array `(n, 96, 64)` where `n` depends on the input audio.\n",
    "\n",
    "At index time we will index each segment to a vector `(n, 96, 64)` will be converted to\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "(0, 96, 64)   ->  (96, 64)  -> embedding\n",
    "(1, 96, 64)   ->  (96, 64)  -> embedding\n",
    "...\n",
    "(n-1, 96, 64) ->  (96, 64)  -> embedding\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then given a query with shape `(n_q, 96,64)` we will create an embedding for each chunk in the query\n",
    "\n",
    "\n",
    "```\n",
    "(0, 96, 64)     ->  (96, 64)  -> embedding\n",
    "(1, 96, 64)     ->  (96, 64)  -> embedding\n",
    "...\n",
    "(n_q-1, 96, 64) ->  (96, 64)  -> embedding\n",
    "```\n",
    "\n",
    "Afterwards we will find for each chunk embedding which is its closest match.\n",
    "\n",
    "Given a chunk from the query $c_k$ we will compute\n",
    "\n",
    "```\n",
    "d(c_k, c) for each c \n",
    "```\n",
    "\n",
    "Then we will get the 'hitted' parents\n",
    "\n",
    "```\n",
    "parent_id_1 : [0.1, 0.3]\n",
    "parent_id_2 : [0.3, 0.4]\n",
    "...\n",
    "parent_id_N : [0.1]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-28T17:43:44.496772Z",
     "start_time": "2021-05-28T17:43:44.450246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @requests(on='/search')\n",
      "    def read(self, docs, *args, **kwargs):\n",
      "        for doc in docs:\n",
      "            data, sample_rate = self.read_wav(doc.uri)\n",
      "            mel_data = self.wav2mel(data, sample_rate)\n",
      "            doc.blob = mel_data[0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(segmenter.read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = vggish_input.wavfile_to_examples('data/Beethoven_1.wav')\n",
    "data_2 = vggish_input.wavfile_to_examples('data/Beethoven_2.wav')\n",
    "print(data_1.shape)\n",
    "print(data_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = executors.VggishSegmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(806912,)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, sample_rate = segmenter.read_wav('data/Beethoven_1.wav')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1409024,)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, sample_rate = segmenter.read_wav('data/Beethoven_2.wav')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing data thorugh the vvgish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executors import VggishEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that different wav files will be represented with different numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 96, 64)\n",
      "(33, 96, 64)\n"
     ]
    }
   ],
   "source": [
    "data_1 = vggish_input.wavfile_to_examples('data/Beethoven_1.wav')\n",
    "data_2 = vggish_input.wavfile_to_examples('data/Beethoven_2.wav')\n",
    "print(data_1.shape)\n",
    "print(data_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a VggishEncoder and look at how to encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:329: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davidbuchaca/Documents/git_stuff/neuralsearch/jina_2/audio-search/models/vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "encoder = VggishEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/davidbuchaca/Documents/git_stuff/neuralsearch/jina_2/audio-search/executors.py'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcefile(encoder._encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at how data is encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _encode(self, docs: DocumentArray, *args, **kwargs):\n",
      "        blobs = docs.get_attributes('blob')\n",
      "        [embedding_batch] = self.sess.run([self.embedding_tensor],\n",
      "                                           feed_dict={self.feature_tensor: blobs})\n",
      "        result = self.post_processor.postprocess(embedding_batch)\n",
      "        embedding_matrix = (np.float32(result) - 128.) / 128.\n",
      "        \n",
      "        for d,e in zip(docs, embedding_matrix):\n",
      "            d.embedding = e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(encoder._encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method VggishEncoder._encode of <executors.VggishEncoder object at 0x172aed850>>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder._encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = Document()\n",
    "d2 = Document()\n",
    "\n",
    "d1.blob = data_1\n",
    "d2.blob = data_2\n",
    "\n",
    "darray = DocumentArray([d1,d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19, 96, 64), (33, 96, 64))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.blob.shape, d2.blob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = darray.get_attributes('blob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-db7c9b3babfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/git_stuff/neuralsearch/jina_2/audio-search/executors.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(self, docs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDocumentArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mblobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'blob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         [embedding_batch] = self.sess.run([self.embedding_tensor],\n\u001b[0m\u001b[1;32m    105\u001b[0m                                            feed_dict={self.feature_tensor: blobs})\n\u001b[1;32m    106\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "encoder._encode(darray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VggishEncoder' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-58bf169c0271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'VggishEncoder' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
