{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brown-purpose",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "upper-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vggish import mel_features\n",
    "from vggish import vggish_input\n",
    "from vggish import vggish_slim\n",
    "\n",
    "import numpy as np\n",
    "import inspect\n",
    "\n",
    "import jina\n",
    "from jina import Document, DocumentArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-thomas",
   "metadata": {},
   "source": [
    "### Prepare vvgish input manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "second-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brown-partnership",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/librosa/core/audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    }
   ],
   "source": [
    "x_audio, sample_rate = librosa.load('data/sample.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "complex-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mel_examples = vggish_input.waveform_to_examples(x_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "expected-greek",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 96, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_mel_examples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-gothic",
   "metadata": {},
   "source": [
    "### We can directly go from path of mp3 file to input vggish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "herbal-bosnia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/librosa/core/audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    }
   ],
   "source": [
    "log_mel_examples = vggish_input.mp3_to_examples('data/sample.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "filled-prince",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 96, 64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_mel_examples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "attractive-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mel_examples = vggish_input.wavfile_to_examples('data/Beethoven_1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "desirable-chester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19, 96, 64), (116736,))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_mel_examples.shape, log_mel_examples.flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-harvest",
   "metadata": {},
   "source": [
    "## Segmenting the data\n",
    "\n",
    "The segmenter in audio-example from jina examples does the following at segment time:\n",
    "\n",
    "- uses `read_wav(path_to_wav)` reads a wav file and returns a numpy array (`data`) and a integer (`sample_rate`)\n",
    "\n",
    "- \n",
    "\n",
    "To read the data the code uses `read_wav` to read the data from a path.\n",
    "This function calls `soundfile.read` which returns a numpy array `data`.\n",
    "\n",
    "\n",
    "\n",
    "Example for `data/Beethoven_1.wav`:\n",
    "\n",
    "- data after `soundfile.read` has shape (806912,)\n",
    "\n",
    "- `mel_data=wav2vel(data)` has shape (806912,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "simplified-transport",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806912, 2)\n",
      "(806912,)\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "wav_data, sample_rate = sf.read('data/Beethoven_1.wav', dtype='int16')\n",
    "print(wav_data.shape)\n",
    "wav_data = np.mean(wav_data, axis=1)\n",
    "data = wav_data / sample_rate\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "induced-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 96, 64)\n",
      "(19, 96, 64)\n"
     ]
    }
   ],
   "source": [
    "from vggish.vggish_input import waveform_to_examples\n",
    "\n",
    "aux = waveform_to_examples(data, sample_rate)\n",
    "print(aux.shape)\n",
    "\n",
    "aux = waveform_to_examples(data, sample_rate).squeeze()\n",
    "print(aux.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "successful-romance",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def segment(self, docs, *args, **kwargs):\n",
      "\n",
      "        for doc in docs:\n",
      "            data, sample_rate = self.read_wav(doc.uri)\n",
      "            mel_data = self.wav2mel(data, sample_rate)\n",
      "            for idx, blob in enumerate(mel_data):\n",
      "                #self.logger.debug(f'blob: {blob.shape}')\n",
      "                doc.chunks.append(Document(offset=idx, weight=1.0, blob=blob))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(segmenter.segment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-interaction",
   "metadata": {},
   "source": [
    "Note that each chunk is a Document containing each element from mel_data.\n",
    "\n",
    "In particular this will create as elements as mel_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "atomic-council",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 64)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-trainer",
   "metadata": {},
   "source": [
    "### Finding matches in a dataset \n",
    "\n",
    "We have seen that given an audio input we create a numpy array `(n, 96, 64)` where `n` depends on the input audio.\n",
    "\n",
    "At index time we will index each segment to a vector `(n, 96, 64)` will be converted to\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "(0, 96, 64)   ->  (96, 64)  -> embedding\n",
    "(1, 96, 64)   ->  (96, 64)  -> embedding\n",
    "...\n",
    "(n-1, 96, 64) ->  (96, 64)  -> embedding\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-surge",
   "metadata": {},
   "source": [
    "Then given a query with shape `(n_q, 96,64)` we will create an embedding for each chunk in the query\n",
    "\n",
    "\n",
    "```\n",
    "(0, 96, 64)     ->  (96, 64)  -> embedding\n",
    "(1, 96, 64)     ->  (96, 64)  -> embedding\n",
    "...\n",
    "(n_q-1, 96, 64) ->  (96, 64)  -> embedding\n",
    "```\n",
    "\n",
    "Afterwards we will find for each chunk embedding which is its closest match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "behind-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @requests(on='/search')\n",
      "    def read(self, docs, *args, **kwargs):\n",
      "        for doc in docs:\n",
      "            data, sample_rate = self.read_wav(doc.uri)\n",
      "            mel_data = self.wav2mel(data, sample_rate)\n",
      "            doc.blob = mel_data[0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(segmenter.read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-breakdown",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-tactics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "environmental-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = vggish_input.wavfile_to_examples('data/Beethoven_1.wav')\n",
    "data_2 = vggish_input.wavfile_to_examples('data/Beethoven_2.wav')\n",
    "print(data_1.shape)\n",
    "print(data_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "outer-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = executors.VggishSegmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "injured-extra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(806912,)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, sample_rate = segmenter.read_wav('data/Beethoven_1.wav')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "canadian-brain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1409024,)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, sample_rate = segmenter.read_wav('data/Beethoven_2.wav')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-journalism",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "imposed-expert",
   "metadata": {},
   "source": [
    "### Passing data thorugh the vvgish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-rubber",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "tribal-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "from executors import VggishEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-positive",
   "metadata": {},
   "source": [
    "Note that different wav files will be represented with different numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "closing-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 96, 64)\n",
      "(33, 96, 64)\n"
     ]
    }
   ],
   "source": [
    "data_1 = vggish_input.wavfile_to_examples('data/Beethoven_1.wav')\n",
    "data_2 = vggish_input.wavfile_to_examples('data/Beethoven_2.wav')\n",
    "print(data_1.shape)\n",
    "print(data_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-graham",
   "metadata": {},
   "source": [
    "Now let's define a VggishEncoder and look at how to encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pursuant-viewer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:329: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davidbuchaca/Documents/git_stuff/neuralsearch/jina_2/audio-search/models/vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "encoder = VggishEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "functioning-think",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/davidbuchaca/Documents/git_stuff/neuralsearch/jina_2/audio-search/executors.py'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcefile(encoder._encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-worship",
   "metadata": {},
   "source": [
    "We can take a look at how data is encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "medium-tutorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _encode(self, docs: DocumentArray, *args, **kwargs):\n",
      "        blobs = docs.get_attributes('blob')\n",
      "        [embedding_batch] = self.sess.run([self.embedding_tensor],\n",
      "                                           feed_dict={self.feature_tensor: blobs})\n",
      "        result = self.post_processor.postprocess(embedding_batch)\n",
      "        embedding_matrix = (np.float32(result) - 128.) / 128.\n",
      "        \n",
      "        for d,e in zip(docs, embedding_matrix):\n",
      "            d.embedding = e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(encoder._encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "affecting-return",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method VggishEncoder._encode of <executors.VggishEncoder object at 0x172aed850>>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder._encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "continuing-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = Document()\n",
    "d2 = Document()\n",
    "\n",
    "d1.blob = data_1\n",
    "d2.blob = data_2\n",
    "\n",
    "darray = DocumentArray([d1,d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "friendly-precipitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19, 96, 64), (33, 96, 64))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.blob.shape, d2.blob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ethical-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = darray.get_attributes('blob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "tribal-fitting",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-db7c9b3babfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/git_stuff/neuralsearch/jina_2/audio-search/executors.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(self, docs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDocumentArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mblobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'blob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         [embedding_batch] = self.sess.run([self.embedding_tensor],\n\u001b[0m\u001b[1;32m    105\u001b[0m                                            feed_dict={self.feature_tensor: blobs})\n\u001b[1;32m    106\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "encoder._encode(darray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ranking-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-result",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collect-twenty",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VggishEncoder' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-58bf169c0271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'VggishEncoder' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-somewhere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-rental",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
